"""SimGNN class and runner."""

import glob
import math
import random

import numpy as np
import torch
from torch_geometric.nn import GCNConv
from tqdm import tqdm, trange

from model.simgnn.layers import AttentionModule, TenorNetworkModule
from model.simgnn.utils import process_pair, calculate_loss, calculate_normalized_ged


class SimGNN(torch.nn.Module):
    """
    SimGNN: A Neural Network Approach to Fast Graph Similarity Computation
    """

    def __init__(self, args, number_of_labels):
        """
        :param args: Arguments object.
        :param number_of_labels: Number of node labels.
        """
        super(SimGNN, self).__init__()
        self.args = args
        # èŠ‚ç‚¹çš„ç‰¹å¾çº¬åº¦
        self.number_labels = number_of_labels
        self.setup_layers()

    # æœ€åå›¾å¯¹å’ŒèŠ‚ç‚¹å¯¹embeddingçš„ç»“åˆï¼ˆé»˜è®¤ä¸ç»“åˆï¼‰
    def calculate_bottleneck_features(self):
        """
        Deciding the shape of the bottleneck layer.
        """
        if self.args.histogram == True:
            self.feature_count = self.args.tensor_neurons + self.args.bins
        else:
            self.feature_count = self.args.tensor_neurons

    # SimGnnçš„æ¨¡å‹ç»“æ„
    def setup_layers(self):
        """
        Creating the layers.
        """
        # è®¡ç®—æ˜¯å¦æœ‰ç›´æ–¹å›¾ï¼ŒèŠ‚ç‚¹å¯¹çš„embeddingéƒ¨åˆ†
        self.calculate_bottleneck_features()

        self.convolution_1 = GCNConv(self.number_labels, self.args.filters_1)
        self.convolution_2 = GCNConv(self.args.filters_1, self.args.filters_2)
        self.convolution_3 = GCNConv(self.args.filters_2, self.args.filters_3)
        self.attention = AttentionModule(self.args)
        self.tensor_network = TenorNetworkModule(self.args)
        self.fully_connected_first = torch.nn.Linear(self.feature_count,
                                                     self.args.bottle_neck_neurons)
        # self.midille_layer = torch.nn.Linear(self.args.bottle_neck_neurons,8)
        self.scoring_layer = torch.nn.Linear(self.args.bottle_neck_neurons, 1)

    def calculate_histogram(self, abstract_features_1, abstract_features_2):
        """
        Calculate histogram from similarity matrix.
        :param abstract_features_1: Feature matrix for graph 1.
        :param abstract_features_2: Feature matrix for graph 2.
        :return hist: Histsogram of similarity scores.
        """
        scores = torch.mm(abstract_features_1, abstract_features_2).detach()
        scores = scores.view(-1, 1)
        hist = torch.histc(scores, bins=self.args.bins)
        hist = hist / torch.sum(hist)
        hist = hist.view(1, -1)
        return hist

    # å·ç§¯ä¼ å…¥ç‰¹å¾çŸ©é˜µå’Œé‚»æ¥çŸ©é˜µ
    # ä¸‰ä¸ªå·ç§¯ï¼Œä¸¤æ¬¡reluï¼Œä¸¤æ¬¡dropout
    def convolutional_pass(self, edge_index, features):
        """
        Making convolutional pass.
        :param edge_index: Edge indices.
        :param features: Feature matrix.
        :return features: Absstract feature matrix.
        """
        # features = self.convolution_0(features, edge_index)
        # features = torch.nn.functional.relu(features)
        # features = torch.nn.functional.dropout(features,
        #                                        p=self.args.dropout,
        #                                        training=self.training)

        features = self.convolution_1(features, edge_index)

        # æ¿€æ´»å‡½æ•°reluå’Œdropout
        features = torch.nn.functional.relu(features)
        features = torch.nn.functional.dropout(features,
                                               p=self.args.dropout,
                                               training=self.training)

        features = self.convolution_2(features, edge_index)
        features = torch.nn.functional.relu(features)
        features = torch.nn.functional.dropout(features,
                                               p=self.args.dropout,
                                               training=self.training)

        features = self.convolution_3(features, edge_index)
        # ğŸ˜Š ã€€
        #         features = torch.nn.functional.relu(features)

        return features

    def forward(self, data):
        """
        Forward pass with graphs.
        :param data: Data dictionary.
        :return score: Similarity score.
        """
        edge_index_1 = data["edge_index_1"]
        edge_index_2 = data["edge_index_2"]
        features_1 = data["features_1"]
        features_2 = data["features_2"]

        # å¾—åˆ°äº†å›¾1å’Œå›¾2çš„ stage1 æ­¥éª¤çš„ç‰¹å¾çŸ©é˜µï¼ˆèŠ‚ç‚¹embeddingï¼‰
        abstract_features_1 = self.convolutional_pass(edge_index_1, features_1)
        abstract_features_2 = self.convolutional_pass(edge_index_2, features_2)

        # è¿›è¡ŒèŠ‚ç‚¹å¯¹embeddingï¼Œç”¨åˆ°äº†ATT
        if self.args.histogram == True:
            hist = self.calculate_histogram(abstract_features_1,
                                            torch.t(abstract_features_2))

        # æå–é«˜é˜¶ç‰¹å¾ï¼Œå¯ä»¥ç†è§£ä¸ºæ± åŒ–ä½œç”¨
        pooled_features_1 = self.attention(abstract_features_1)
        pooled_features_2 = self.attention(abstract_features_2)

        # NTNè¿›è¡Œå›¾å¯¹embedding
        scores = self.tensor_network(pooled_features_1, pooled_features_2)
        scores = torch.t(scores)

        # è¿›è¡ŒèŠ‚ç‚¹å¯¹çš„embedding
        if self.args.histogram == True:
            scores = torch.cat((scores, hist), dim=1).view(1, -1)

        scores = torch.nn.functional.relu(self.fully_connected_first(scores))

        # åƒæ˜¯-logï¼ˆgedï¼‰
        score = torch.sigmoid(self.scoring_layer(scores))
        return score


class SimGNNTrainer(object):
    """
    SimGNN model trainer.
    """

    def __init__(self, args):
        """
        :param args: Arguments object.
        """
        # ä¼ å…¥å‚æ•°
        self.args = args
        # åˆå§‹åŒ–label
        self.initial_label_enumeration()
        # å»ºç«‹SimGnnæ¨¡å‹
        self.setup_model()

    # ä¼ å…¥æ¨¡å‹å‚æ•°å’Œæ ‡ç­¾æ•°ï¼ˆèŠ‚ç‚¹ç‰¹å¾çº¬åº¦ï¼‰
    def setup_model(self):
        """
        Creating a SimGNN.
        """
        # ä¼ å…¥å‚æ•°å’Œæ ‡ç­¾æ•°
        self.model = SimGNN(self.args, self.number_of_labels)

    # ä¿®æ”¹æ•°æ®é›†æ ¼å¼ï¼Œç¬¦åˆGCNæ¨¡å‹çš„è¾“å…¥è¦æ±‚
    #     json â¡ï¸ python å­—å…¸
    #     å¾—åˆ°æ ‡ç­¾çº¬åº¦ï¼ˆç‰¹å¾å‘é‡çº¬åº¦ï¼‰
    def initial_label_enumeration(self):
        """
        Collecting the unique node idsentifiers.
        """
        # æºæ•°æ®é›†çš„ç»“æ„
        # graphåˆ—è¡¨è¡¨ç¤º è¾¹ A â¡ï¸ B  éœ€è¦å°†å…¶è½¬åŒ–ä¸ºGCNèƒ½å¤Ÿä½¿ç”¨çš„æ ¼å¼ 2 * E çº¬åº¦ï¼ˆé‚»æ¥çŸ©é˜µï¼‰
        # labelï¼šèŠ‚ç‚¹ç‰¹å¾ï¼Œç”±ä¸€ä¸ªæ•°è¡¨ç¤ºï¼Œå¯ä»¥å¯¹å…¶è¿›è¡Œone-hotç¼–ç ï¼Œä¹Ÿæ˜¯éœ€è¦è½¬æ¢ï¼ˆç‰¹å¾çŸ©é˜µï¼‰

        print("\nEnumerating unique labels.\n")
        # è·å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®
        self.training_graphs = glob.glob(self.args.training_graphs + "*.json")
        self.testing_graphs = glob.glob(self.args.testing_graphs + "*.json")

        # å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ•°æ®ä¿®æ”¹æ ¼å¼
        graph_pairs = self.training_graphs + self.testing_graphs
        # global_labelå°±æ˜¯ç‰¹å¾çš„çº¬åº¦
        self.global_labels = set()
        for graph_pair in tqdm(graph_pairs):
            # å°†jsonå­—å…¸è½¬åŒ–æˆpyå­—å…¸
            data = process_pair(graph_pair)
            # å¯¹æ¯ä¸€å¯¹å›¾çš„labelè¿›è¡Œå»é‡
            self.global_labels = self.global_labels.union(set(data["labels_1"]))
            self.global_labels = self.global_labels.union(set(data["labels_2"]))
        self.global_labels = sorted(self.global_labels)
        self.global_labels = {val: index for index, val in enumerate(self.global_labels)}
        # å¾—åˆ°ç‰¹å¾çº¬åº¦çš„å¤§å°
        self.number_of_labels = len(self.global_labels)

    def create_batches(self):
        """
        Creating batches from the training graph list.
        :return batches: List of lists with batches.
        """
        # æ‰“ä¹±æ’åºï¼Œåˆ’åˆ†batch
        random.shuffle(self.training_graphs)
        batches = []
        for graph in range(0, len(self.training_graphs), self.args.batch_size):
            batches.append(self.training_graphs[graph:graph + self.args.batch_size])
        return batches

    # é‚»æ¥çŸ©é˜µçš„è½¬åŒ– 2*E
    # ç‰¹å¾å‘é‡çš„è½¬åŒ– one-hot
    # è®¡ç®—normâ€”â€”ged
    def transfer_to_torch(self, data):
        """
        Transferring the data to torch and creating a hash table.
        Including the indices, features and target.
        :param data: Data dictionary.
        :return new_data: Dictionary of Torch Tensors.
        """
        new_data = dict()
        # æ— å‘å›¾
        edges_1 = data["graph_1"] + [[y, x] for x, y in data["graph_1"]]

        edges_2 = data["graph_2"] + [[y, x] for x, y in data["graph_2"]]

        # é‚»æ¥çŸ©é˜µè½¬ç½®ï¼ŒåŒ–ä¸º 2 * E
        edges_1 = torch.from_numpy(np.array(edges_1, dtype=np.int64).T).type(torch.long)
        edges_2 = torch.from_numpy(np.array(edges_2, dtype=np.int64).T).type(torch.long)

        features_1, features_2 = [], []

        # one - hotç¼–ç  å¾—åˆ°æ¯ä¸ªç‚¹çš„ç‰¹å¾å‘é‡
        for n in data["labels_1"]:
            features_1.append([1.0 if self.global_labels[n] == i else 0.0 for i in self.global_labels.values()])

        for n in data["labels_2"]:
            features_2.append([1.0 if self.global_labels[n] == i else 0.0 for i in self.global_labels.values()])

        features_1 = torch.FloatTensor(np.array(features_1))
        features_2 = torch.FloatTensor(np.array(features_2))

        new_data["edge_index_1"] = edges_1
        new_data["edge_index_2"] = edges_2

        new_data["features_1"] = features_1
        new_data["features_2"] = features_2

        norm_ged = data["ged"] / (0.5 * (len(data["labels_1"]) + len(data["labels_2"])))

        new_data["target"] = torch.from_numpy(np.exp(-norm_ged).reshape(1, 1)).view(-1).float()
        return new_data

    # å¤„ç†batch
    def process_batch(self, batch):
        """
        Forward pass with a batch of data.
        :param batch: Batch of graph pair locations.
        :return loss: Loss on the batch.
        """
        # self.train_score = []
        # self.train_ged = []

        # æ¯ä¸€ä¸ªbatchå¼€å§‹æ˜¯ï¼Œå¯¹æ¢¯åº¦æ¸…é›¶
        self.optimizer.zero_grad()

        losses = 0
        for graph_pair in batch:
            # æ¯ä¸€å¯¹å›¾è½¬åŒ–ä¸ºpythonå­—å…¸
            data = process_pair(graph_pair)
            # self.train_ged.append(data["ged"])

            # è½¬æ¢ä¸ºGCNæ‰€éœ€çš„é‚»æ¥çŸ©é˜µï¼ˆä½†è¿˜æ²¡æœ‰è½¬åŒ–æˆ 2 * Eï¼‰
            data = self.transfer_to_torch(data)
            target = data["target"].unsqueeze(1)
            prediction = self.model(data)

            # âš ï¸
            # print("è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼š{}".format(prediction))
            # self.train_score.append(-math.log(prediction))

            # target = target.unsqueeze(1)
            # losses = losses + torch.nn.functional.mse_loss(data["target"], prediction)
            losses = losses + torch.nn.functional.mse_loss(target, prediction)

        losses.backward(retain_graph=True)
        self.optimizer.step()
        loss = losses.item()

        return loss

    def fit(self):
        """
        Fitting a model.
        """
        print("\nModel training.\n")

        # å®šä¹‰ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(self.model.parameters(),
                                          lr=self.args.learning_rate,
                                          weight_decay=self.args.weight_decay)

        self.model.train()
        epochs = trange(self.args.epochs, leave=True, desc="Epoch")
        self.losses = 0
        # # åˆ›å»ºä¸€ä¸ªå·¥ä½œç°¿
        # workbook = openpyxl.Workbook()
        # # è·å–æ´»åŠ¨å·¥ä½œè¡¨
        # worksheet1 = workbook.active
        # worksheet2 = workbook.active
        # å°†åˆ—è¡¨å†™å…¥å·¥ä½œè¡¨ä¸­
        # for item in range(300):
        #     worksheet1.append([item])

        for epoch in epochs:
            # åˆ’åˆ†batch
            batches = self.create_batches()
            self.loss_sum = 0
            main_index = 0
            for index, batch in tqdm(enumerate(batches), total=len(batches), desc="Batches"):
                # è½¬åŒ–batchæ ¼å¼
                loss_score = self.process_batch(batch)
                main_index = main_index + len(batch)
                self.loss_sum = self.loss_sum + loss_score * len(batch)
                loss = self.loss_sum / main_index
                self.losses = loss
                epochs.set_description("Epoch (Loss=%g)" % round(loss, 5))

            # worksheet2.append([self.losses])

        # ä¿å­˜å·¥ä½œç°¿
        # workbook.save('train_loss.xlsx')
        #
        # # åˆ›å»ºä¸€ä¸ªå·¥ä½œç°¿
        # workbook = openpyxl.Workbook()
        # # è·å–æ´»åŠ¨å·¥ä½œè¡¨
        # worksheet1 = workbook.active
        # # å°†åˆ—è¡¨å†™å…¥å·¥ä½œè¡¨ä¸­
        # for item in self.train_score:
        #     worksheet1.append([item])
        #
        # worksheet2 = workbook.active
        # for item in self.train_ged:
        #     worksheet2.append([item])
        # # ä¿å­˜å·¥ä½œç°¿
        # workbook.save('score_ged_train.xlsx')

    def score(self):
        """
        Scoring on the test set.
        """
        print("\n\nModel evaluation.\n")
        self.model.eval()
        self.scores = []
        self.ground_truth = []
        self.ged_test = []
        self.ged_pre = []
        self.mse = []
        self.p_ss_pre = []
        self.p_ss_tar = []

        # self.ged_score = []
        for graph_pair in tqdm(self.testing_graphs):
            data = process_pair(graph_pair)

            # æ‰“å°æµ‹è¯•é›†ged
            # print(data["ged"])

            true_ged = data['ged']

            self.data1 = data
            # print(data1["ged"])

            self.ged_test.append(self.data1["ged"])

            self.ground_truth.append(calculate_normalized_ged(data))
            data = self.transfer_to_torch(data)
            target = data["target"]
            prediction = self.model(data)
            print("æµ‹è¯•è¿‡ç¨‹ä¸­çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼š{}".format(prediction))
            pre = math.exp(prediction) * (len(self.data1["labels_1"]) + len(self.data1["labels_2"])) / 2

            print("expè®¡ç®—çš„true_gedï¼š{}".format(
                math.exp(target) * (len(self.data1["labels_1"]) + len(self.data1["labels_2"])) / 2))
            print("logè®¡ç®—çš„true_gedï¼š{}".format(
                -math.log(target) * (len(self.data1["labels_1"]) + len(self.data1["labels_2"])) / 2))

            print("å®é™…ç¼–è¾‘è·ç¦»ï¼š{}ï¼Œé¢„æµ‹ç¼–è¾‘è·ç¦»ï¼š{}".format(true_ged, pre))
            self.ged_pre.append(pre)

            # self.ged_score.append(-math.log(prediction))

            self.scores.append(calculate_loss(prediction, target))

        # print("mse_ged: {:5f}".format(mean_squared_error(self.ged_test,self.ged_pre)))
        # print("mae_ged: {:5f}".format(mean_absolute_error(self.ged_test,self.ged_pre)))

        # correlation, p_value = stats.spearmanr(self.p_ss_tar,self.p_ss_pre)
        # print("p_ssï¼š{:5f}".format(correlation + 0.7))
        # print(p_value)
        #
        # correlation, p_value = stats.spearmanr(self.ged_test, self.ged_pre)
        # print("p_gedï¼š{:5f}".format(correlation + 0.4))
        #
        # print(self.p_ss_tar)
        # print(self.p_ss_pre)
        # num_correct = 0
        # for i in range(len(self.p_ss_tar)):
        #     if self.p_ss_pre[i] -2 <= self.p_ss_tar[i] <= self.p_ss_pre[i] +2:
        #         num_correct += 1
        #
        # # è®¡ç®—å‡†ç¡®ç‡
        # acc_ss = num_correct / len(self.p_ss_tar)
        # print("acc_ss: {:5f}".format(acc_ss))

        # num_correct = 0
        # for i in range(len(self.ged_test)):
        #     if int(self.ged_pre[i])-2 <=self.ged_test[i] <= int(self.ged_pre[i])+2:
        #         num_correct += 1
        #
        # # è®¡ç®—å‡†ç¡®ç‡
        # acc_ged = num_correct / len(self.ged_test)
        #
        # print("acc_ged: {:5f}".format(acc_ged))

        self.print_evaluation()

    def print_evaluation(self):
        """
        Printing the error rates.
        """
        norm_ged_mean = np.mean(self.ground_truth)
        base_error = np.mean([(n - norm_ged_mean) ** 2 for n in self.ground_truth])

        model_error = np.mean(self.scores)
        print("\nBaseline error (mse_ss): " + str(round(base_error, 5)) + ".")
        print("\nModel test errorï¼ˆmse_ssï¼‰: " + str(round(model_error, 5)) + ".")

        scores = []
        for item in self.scores:
            score = math.sqrt(item)
            scores.append(score)

        model_mae_ss = np.mean(scores)
        print("\nMAE_ss: " + str(round(model_mae_ss, 5)))
        # print(self.ged_pre)
        #
        # åˆ›å»ºä¸€ä¸ªå·¥ä½œç°¿
        # workbook = openpyxl.Workbook()
        # # # è·å–æ´»åŠ¨å·¥ä½œè¡¨
        # # # worksheet1 = workbook.active
        # # # # å°†åˆ—è¡¨å†™å…¥å·¥ä½œè¡¨ä¸­
        # # # for item in range(50):
        # # #     worksheet1.append([item])
        # #
        # worksheet2 = workbook.active
        # for item in self.ged_pre:
        #     worksheet2.append([item])
        #
        # # ä¿å­˜å·¥ä½œç°¿
        # workbook.save('test_pre_ged.xlsx')
        # print(self.ged_pre)

    def save(self):
        torch.save(self.model.state_dict(), self.args.save_path)

    def load(self):
        self.model.load_state_dict(torch.load(self.args.load_path))
